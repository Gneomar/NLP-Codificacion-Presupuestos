{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('Data/data_clean.csv')\n",
    "\n",
    "col_target = 'Codigo'\n",
    "col_features = [col for col in data.columns if col != col_target]\n",
    "\n",
    "X = data[col_features]\n",
    "y = data[col_target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('Data/data_clean.csv')\n",
    "\n",
    "# Paso 1: Codificación de la columna 'Codigo'\n",
    "label_encoder = LabelEncoder()\n",
    "data['Codigo_encoded'] = label_encoder.fit_transform(data['Codigo'])\n",
    "\n",
    "# Paso 2: Tokenización y padding de la columna 'Descripcion'\n",
    "max_words = 10000  # Número máximo de palabras que el tokenizador considerará\n",
    "max_len = 100  # Longitud máxima de las secuencias\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['Descripcion'])\n",
    "\n",
    "# Convertir las descripciones a secuencias de números\n",
    "X_seq = tokenizer.texts_to_sequences(data['Descripcion'])\n",
    "\n",
    "# Aplicar padding para que todas las secuencias tengan la misma longitud\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
    "\n",
    "# Ahora X_pad es tu conjunto de características y 'Codigo_encoded' tu conjunto de etiquetas\n",
    "X = X_pad\n",
    "y = data['Codigo_encoded']\n",
    "\n",
    "# División del conjunto en entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Ahora ya puedes usar este dataset para entrenar el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "\n",
    "# Parámetros\n",
    "embedding_dim = 64  # Dimensión de los embeddings (puedes ajustarla)\n",
    "\n",
    "# Construcción del modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de Embedding: convierte palabras en vectores\n",
    "model.add(Embedding(input_dim=10000, output_dim=embedding_dim))\n",
    "\n",
    "# Capa de GlobalAveragePooling: reduce las dimensiones\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Primera capa densa\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Capa de salida: tantas neuronas como clases (etiquetas/códigos)\n",
    "model.add(Dense(len(set(y_train)), activation='softmax'))\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Precisión en el conjunto de prueba: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "data = pd.read_csv('Data/data_clean.csv')\n",
    "\n",
    "# Definir las columnas objetivo y características\n",
    "col_target = 'Codigo'\n",
    "col_features = ['Descripcion']  # Nos centramos en la columna 'Descripcion'\n",
    "\n",
    "X = data[col_features]\n",
    "y = data[col_target]\n",
    "\n",
    "# Paso 1: Codificación de las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Paso 2: Tokenización y Padding de las descripciones\n",
    "max_words = 10000  # Número máximo de palabras en el vocabulario del tokenizer\n",
    "max_len = 100  # Longitud máxima de las secuencias\n",
    "\n",
    "# Crear el tokenizador y ajustarlo en el texto de las descripciones\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X['Descripcion'])\n",
    "\n",
    "# Convertir el texto en secuencias numéricas\n",
    "X_seq = tokenizer.texts_to_sequences(X['Descripcion'])\n",
    "\n",
    "# Aplicar padding para que todas las secuencias tengan la misma longitud\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
    "\n",
    "# Paso 3: División del conjunto en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded, test_size=0.2, stratify=y_encoded)\n",
    "\n",
    "# Paso 4: Construcción del modelo\n",
    "embedding_dim = 64  # Tamaño de los embeddings\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de Embedding\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# Capa GlobalAveragePooling1D para reducir la dimensionalidad\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Capa completamente conectada con 64 neuronas y activación ReLU\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Capa de salida con tantas neuronas como clases y activación softmax\n",
    "model.add(Dense(len(set(y_encoded)), activation='softmax'))\n",
    "\n",
    "# Compilación del modelo con el optimizador Adam y pérdida categórica\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mostrar el resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Paso 5: Entrenamiento del modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "# Paso 6: Evaluación del modelo\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Precisión en el conjunto de prueba: {test_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
